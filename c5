#9
from datetime import datetime

# file format level, save all dataframes
all_dfs = []

adls_mnt_directory_path_formatted = adls_mnt_directory_path_from_metadata_table

# save all logs for metatable purpose
log_metadata = []

# adls_mnt_directory_path_formatted ="/Volumes/bnlwe_ai_foundation_rag_dev/innoflex_bnw_hr/akshay/french_all_formats/"

client = authenticate_text_analytics_client()

translate_supported_languages = text_translator.get_languages()
translate_supported_languages.keys()
list_of_translation_supported_languages = list(translate_supported_languages['translation'].keys())
print(list_of_translation_supported_languages)

for filename in os.listdir(adls_mnt_directory_path_formatted):
    print(f" ********* processing file: {filename} **************** ")
    if filename.lower().endswith(".pdf"):
        # print(os.path.join(adls_mnt_directory_path_formatted, filename))
        start_time = datetime.now()

        # parse content from DI
        # shub - check if the file is corrupt before parsing
        if os.path.getsize(os.path.join(adls_mnt_directory_path_formatted, filename)) > 0:
            filename, documents = parse_pdf_from_document_intelligence(os.path.join(adls_mnt_directory_path_formatted, filename))

            # print(f"documents: {documents}")
            ###########

            # find out language from first and last page
            language_translation_check_text = documents[0].page_content + documents[-1].page_content 
            language_detection_response = azure_ai_service_language_detection(client, language_translation_check_text)
            # print(language_detection_response)
            source_language_shorthand = language_detection_response["iso6391_name"] # fr
            # source_language_fullname = language_detection_response["language"]

            # based on detected language, call translaion
            if source_language_shorthand != "en":
                # print("call prepare_pdf_df")
                parse_pdf_response = prepare_pdf_df(filename, documents, translate=True, source_language_shorthand = source_language_shorthand)
            
            else:
                parse_pdf_response = prepare_pdf_df(filename, documents, translate=False, source_language_shorthand = None)


            ###########        parse_pdf_response = prepare_pdf_df(filename, documents)
            end_time = datetime.now()
            if parse_pdf_response["status_code"] == 200:
                parse_pdf_response["dataframe"] = parse_pdf_response["dataframe"].withColumn("page_number",F.col("page_number").cast(IntegerType()))
                all_dfs.append(parse_pdf_response["dataframe"])
                log_metadata.append({
                    "job_id": job_id,
                    "run_id": run_id,
                    "rag_app_name" : rag_app_name_from_metadata_table,
                    "rag_app_source_id" : rag_app_source_id_from_metadata_table,
                    "source_doc_path" : os.path.join(adls_mnt_directory_path_from_metadata_table, filename),
                    "source_doc_name" : filename,
                    "target_stage" : "silver layer",
                    "target_path": rag_storage_acc_mount_point_from_metadata_table+silver_layer_target_path_from_metadata_table+"created_date="+str(datetime.now().year)+"-"+str(datetime.now().month)+"-"+str(datetime.now().day),
                    "ingestion_status": "success",
                    "start_time" : start_time,
                    "end_time" : end_time
                    })
            else:
                log_metadata.append({
                    "job_id": job_id,
                    "run_id": run_id,
                    "rag_app_name" : rag_app_name_from_metadata_table,
                    "rag_app_source_id" : rag_app_source_id_from_metadata_table,
                    "source_doc_path" : os.path.join(adls_mnt_directory_path_from_metadata_table, filename),
                    "source_doc_name" : filename,
                    "target_stage" : "silver layer",
                    "target_path": rag_storage_acc_mount_point_from_metadata_table+silver_layer_target_path_from_metadata_table+"created_date="+str(datetime.now().year)+"-"+str(datetime.now().month)+"-"+str(datetime.now().day),
                    "ingestion_status": "failure",
                    "error_details": parse_pdf_response["status_details"],
                    "start_time" : start_time,
                    "end_time" : end_time
                })
    elif filename.lower().endswith(".docx"):
        # print(os.path.join(adls_mnt_directory_path_formatted, filename))
        start_time = datetime.now()
        # shub - check if the file is corrupt before parsing
        if os.path.getsize(os.path.join(adls_mnt_directory_path_formatted, filename)) > 0:
            filename, documents = parse_word_or_image_from_document_intelligence(os.path.join(adls_mnt_directory_path_formatted, filename))
            # find out language from few begining text
            # limit 5k because azure service can only process 5120 charac at a time
            language_detection_response = azure_ai_service_language_detection(client, documents[0].page_content[:5000])
            # print(language_detection_response)
            source_language_shorthand = language_detection_response["iso6391_name"]

            # based on detected language, call translaion
            if source_language_shorthand != "en":
                parse_word_response = prepare_word_or_image_df(filename, documents, translate=True, source_language_shorthand = source_language_shorthand)
            else:
                parse_word_response = prepare_word_or_image_df(filename, documents, translate=False, source_language_shorthand = None)
            end_time = datetime.now()
            if parse_word_response["status_code"] == 200:
                all_dfs.append(parse_word_response["dataframe"])
                log_metadata.append({
                    "job_id": job_id,
                    "run_id": run_id,
                    "rag_app_name" : rag_app_name_from_metadata_table,
                    "rag_app_source_id" : rag_app_source_id_from_metadata_table,
                    "source_doc_path" : os.path.join(adls_mnt_directory_path_from_metadata_table, filename),
                    "source_doc_name" : filename,
                    "target_stage" : "silver layer",
                    "target_path": rag_storage_acc_mount_point_from_metadata_table+silver_layer_target_path_from_metadata_table+"created_date="+str(datetime.now().year)+"-"+str(datetime.now().month)+"-"+str(datetime.now().day),
                    "ingestion_status": "success",
                    "start_time" : start_time,
                    "end_time" : end_time
                    })
            else:
                log_metadata.append({
                    "job_id": job_id,
                    "run_id": run_id,
                    "rag_app_name" : rag_app_name_from_metadata_table,
                    "rag_app_source_id" : rag_app_source_id_from_metadata_table,
                    "source_doc_path" : os.path.join(adls_mnt_directory_path_from_metadata_table, filename),
                    "source_doc_name" : filename,
                    "target_stage" : "silver layer",
                    "target_path": rag_storage_acc_mount_point_from_metadata_table+silver_layer_target_path_from_metadata_table+"created_date="+str(datetime.now().year)+"-"+str(datetime.now().month)+"-"+str(datetime.now().day),
                    "ingestion_status": "failure",
                    "error_details": parse_word_response["status_details"],
                    "start_time" : start_time,
                    "end_time" : end_time
                })
    elif filename.lower().endswith(".csv"):
        # print(os.path.join(adls_mnt_directory_path_formatted, filename))
        start_time = datetime.now()
        # shub - check if the file is corrupt before parsing
        if os.path.getsize(os.path.join(adls_mnt_directory_path_formatted, filename)) > 0:
            parse_csv_response = csv_parser(os.path.join(adls_mnt_directory_path_formatted, filename))
            end_time = datetime.now()
            if parse_csv_response["status_code"] == 200:
                parse_csv_response["dataframe"] = parse_csv_response["dataframe"].withColumn("row_number",F.col("row_number").cast(IntegerType()))
                all_dfs.append(parse_csv_response["dataframe"])
                log_metadata.append({
                    "job_id": job_id,
                    "run_id": run_id,
                    "rag_app_name" : rag_app_name_from_metadata_table,
                    "rag_app_source_id" : rag_app_source_id_from_metadata_table,
                    "source_doc_path" : os.path.join(adls_mnt_directory_path_from_metadata_table, filename),
                    "source_doc_name" : filename,
                    "target_stage" : "silver layer",
                    "target_path": rag_storage_acc_mount_point_from_metadata_table+silver_layer_target_path_from_metadata_table+"created_date="+str(datetime.now().year)+"-"+str(datetime.now().month)+"-"+str(datetime.now().day),
                    "ingestion_status": "success",
                    "start_time" : start_time,
                    "end_time" : end_time
                })
            else:
                log_metadata.append({
                    "job_id": job_id,
                    "run_id": run_id,
                    "rag_app_name" : rag_app_name_from_metadata_table,
                    "rag_app_source_id" : rag_app_source_id_from_metadata_table,
                    "source_doc_path" : os.path.join(adls_mnt_directory_path_from_metadata_table, filename),
                    "source_doc_name" : filename,
                    "target_stage" : "silver layer",
                    "target_path": rag_storage_acc_mount_point_from_metadata_table+silver_layer_target_path_from_metadata_table+"created_date="+str(datetime.now().year)+"-"+str(datetime.now().month)+"-"+str(datetime.now().day),
                    "ingestion_status": "failure",
                    "error_details": parse_csv_response["status_details"],
                    "start_time" : start_time,
                    "end_time" : end_time
                })
    elif filename.lower().endswith((".jpg", ".jpeg", ".png")):
        # print(os.path.join(adls_mnt_directory_path_formatted, filename))
        start_time = datetime.now()
        # shub - check if the file is corrupt before parsing
        if os.path.getsize(os.path.join(adls_mnt_directory_path_formatted, filename)) > 0:
            parse_image_response = image_parser(os.path.join(adls_mnt_directory_path_formatted, filename))
            end_time = datetime.now()
            if parse_image_response["status_code"] == 200:
                # parse_image_response["dataframe"] = parse_image_response["dataframe"].withColumn("row_number",F.col("row_number").cast(IntegerType()))
                display(parse_image_response["dataframe"])
                all_dfs.append(parse_image_response["dataframe"])
                log_metadata.append({
                    "job_id": job_id,
                    "run_id": run_id,
                    "rag_app_name" : rag_app_name_from_metadata_table,
                    "rag_app_source_id" : rag_app_source_id_from_metadata_table,
                    "source_doc_path" : os.path.join(adls_mnt_directory_path_from_metadata_table, filename),
                    "source_doc_name" : filename,
                    "target_stage" : "silver layer",
                    "target_path": rag_storage_acc_mount_point_from_metadata_table+silver_layer_target_path_from_metadata_table+"created_date="+str(datetime.now().year)+"-"+str(datetime.now().month)+"-"+str(datetime.now().day),
                    "ingestion_status": "success",
                    "start_time" : start_time,
                    "end_time" : end_time
                })
            else:
                log_metadata.append({
                    "job_id": job_id,
                    "run_id": run_id,
                    "rag_app_name" : rag_app_name_from_metadata_table,
                    "rag_app_source_id" : rag_app_source_id_from_metadata_table,
                    "source_doc_path" : os.path.join(adls_mnt_directory_path_from_metadata_table, filename),
                    "source_doc_name" :filename,
                    "target_stage" : "silver layer",
                    "target_path": rag_storage_acc_mount_point_from_metadata_table+silver_layer_target_path_from_metadata_table+"created_date="+str(datetime.now().year)+"-"+str(datetime.now().month)+"-"+str(datetime.now().day),
                    "ingestion_status": "failure",
                    "error_details": parse_image_response["status_details"],
                    "start_time" : start_time,
                    "end_time" : end_time
                })
    elif filename.lower().endswith((".html", ".HTML")):
        # print(os.path.join(adls_mnt_directory_path_formatted, filename))
        # print("html files are present")
        start_time = datetime.now()
        # shub - check if the file is corrupt before parsing
        if os.path.getsize(os.path.join(adls_mnt_directory_path_formatted, filename)) > 0:
            parse_html_response = process_html_files(os.path.join(adls_mnt_directory_path_formatted, filename),translate=True)
            end_time = datetime.now()
            if parse_html_response["status_code"] == 200:
                # parse_html_response["dataframe"] = parse_html_response["dataframe"].withColumn("row_number",F.col("row_number").cast(IntegerType()))
                all_dfs.append(parse_html_response["dataframe"])
                log_metadata.append({
                    "job_id": job_id,
                    "run_id": run_id,
                    "rag_app_name" : rag_app_name_from_metadata_table,
                    "rag_app_source_id" : rag_app_source_id_from_metadata_table,
                    "source_doc_path" : os.path.join(adls_mnt_directory_path_from_metadata_table, filename),
                    "source_doc_name" : filename,
                    "target_stage" : "silver layer",
                    "target_path": rag_storage_acc_mount_point_from_metadata_table+silver_layer_target_path_from_metadata_table+"created_date="+str(datetime.now().year)+"-"+str(datetime.now().month)+"-"+str(datetime.now().day),
                    "ingestion_status": "success",
                    "start_time" : start_time,
                    "end_time" : end_time
                })
            else:

                # print(parse_html_response["status_details"])
                log_metadata.append({
                    "job_id": job_id,
                    "run_id": run_id,
                    "rag_app_name" : rag_app_name_from_metadata_table,
                    "rag_app_source_id" : rag_app_source_id_from_metadata_table,
                    "source_doc_path" : os.path.join(adls_mnt_directory_path_from_metadata_table, filename),
                    "source_doc_name" : filename,
                    "target_stage" : "silver layer",
                    "target_path": rag_storage_acc_mount_point_from_metadata_table+silver_layer_target_path_from_metadata_table+"created_date="+str(datetime.now().year)+"-"+str(datetime.now().month)+"-"+str(datetime.now().day),
                    "ingestion_status": "failure",
                    "error_details": parse_html_response["status_details"],
                    "start_time" : start_time,
                    "end_time" : end_time
                })

    elif filename.lower().endswith(('.pptx', '.PPTX')):
        # print(os.path.join(adls_mnt_directory_path_formatted, filename))
        start_time = datetime.now()
        # shub - check if the file is corrupt before parsing
        if os.path.getsize(os.path.join(adls_mnt_directory_path_formatted, filename)) > 0:
            parse_ppt_response = process_to_ppt_spark_df(os.path.join(adls_mnt_directory_path_formatted, filename))
            end_time = datetime.now()
            if parse_ppt_response["status_code"] == 200:
                parse_ppt_response["dataframe"] = parse_ppt_response["dataframe"].withColumn("page_number",F.col("page_number").cast(IntegerType()))
                display(parse_ppt_response["dataframe"])
                all_dfs.append(parse_ppt_response["dataframe"])
                log_metadata.append({
                    "job_id": job_id,
                    "run_id": run_id,
                    "rag_app_name" : rag_app_name_from_metadata_table,
                    "rag_app_source_id" : rag_app_source_id_from_metadata_table,
                    "source_doc_path" : os.path.join(adls_mnt_directory_path_from_metadata_table, filename),
                    "source_doc_name" : filename,
                    "target_stage" : "silver layer",
                    "target_path": rag_storage_acc_mount_point_from_metadata_table+silver_layer_target_path_from_metadata_table+"created_date="+str(datetime.now().year)+"-"+str(datetime.now().month)+"-"+str(datetime.now().day),
                    "ingestion_status": "success",
                    "start_time" : start_time,
                    "end_time" : end_time
                })
            else:
                log_metadata.append({
                    "job_id": job_id,
                    "run_id": run_id,
                    "rag_app_name" : rag_app_name_from_metadata_table,
                    "rag_app_source_id" : rag_app_source_id_from_metadata_table,
                    "source_doc_path" : os.path.join(adls_mnt_directory_path_from_metadata_table, filename),
                    "source_doc_name" :filename,
                    "target_stage" : "silver layer",
                    "target_path": rag_storage_acc_mount_point_from_metadata_table+silver_layer_target_path_from_metadata_table+"created_date="+str(datetime.now().year)+"-"+str(datetime.now().month)+"-"+str(datetime.now().day),
                    "ingestion_status": "failure",
                    "error_details": parse_ppt_response["status_details"],
                    "start_time" : start_time,
                    "end_time" : end_time
                })
    elif filename.lower().endswith(".xlsx"):
        # print(os.path.join(adls_mnt_directory_path_formatted, filename))
        start_time = datetime.now()
        # shub - check if the file is corrupt before parsing
        if os.path.getsize(os.path.join(adls_mnt_directory_path_formatted, filename)) > 0:
            parse_excel_response = create_excel_spark_dataframe(os.path.join(adls_mnt_directory_path_formatted, filename),translate=True)
            end_time = datetime.now()
            if parse_excel_response["status_code"] == 200:
                parse_excel_response["dataframe"] = parse_excel_response["dataframe"].withColumn("row_number",F.col("row_number").cast(IntegerType()))
                display(parse_excel_response["dataframe"])
                all_dfs.append(parse_excel_response["dataframe"])
                log_metadata.append({
                    "job_id": job_id,
                    "run_id": run_id,
                    "rag_app_name" : rag_app_name_from_metadata_table,
                    "rag_app_source_id" : rag_app_source_id_from_metadata_table,
                    "source_doc_path" : os.path.join(adls_mnt_directory_path_from_metadata_table, filename),
                    "source_doc_name" : filename,
                    "target_stage" : "silver layer",
                    "target_path": rag_storage_acc_mount_point_from_metadata_table+silver_layer_target_path_from_metadata_table+"created_date="+str(datetime.now().year)+"-"+str(datetime.now().month)+"-"+str(datetime.now().day),
                    "ingestion_status": "success",
                    "start_time" : start_time,
                    "end_time" : end_time
                })
            else:
                log_metadata.append({
                    "job_id": job_id,
                    "run_id": run_id,
                    "rag_app_name" : rag_app_name_from_metadata_table,
                    "rag_app_source_id" : rag_app_source_id_from_metadata_table,
                    "source_doc_path" : os.path.join(adls_mnt_directory_path_from_metadata_table, filename),
                    "source_doc_name" :filename,
                    "target_stage" : "silver layer",
                    "target_path": rag_storage_acc_mount_point_from_metadata_table+silver_layer_target_path_from_metadata_table+"created_date="+str(datetime.now().year)+"-"+str(datetime.now().month)+"-"+str(datetime.now().day),
                    "ingestion_status": "failure",
                    "error_details": parse_excel_response["status_details"],
                    "start_time" : start_time,
                    "end_time" : end_time
                })

    # ============================================================================
    # AUDIO INTEGRATION: ADD AUDIO FILE PROCESSING CONDITION
    # NEW CODE STARTS HERE - ADD AFTER XLSX CONDITION
    # ============================================================================
    elif filename.lower().endswith((".mp3", ".wav", ".m4a")):
        print(f" ********* processing AUDIO file: {filename} **************** ")
        start_time = datetime.now()
        
        # Check if file is not empty
        if os.path.getsize(os.path.join(adls_mnt_directory_path_formatted, filename)) > 0:
            
            # Call our audio processing function
            parse_audio_response = process_audio_file(os.path.join(adls_mnt_directory_path_formatted, filename))
            
            end_time = datetime.now()
            
            # Handle success case
            if parse_audio_response["status_code"] == 200:
                parse_audio_response["dataframe"] = parse_audio_response["dataframe"].withColumn("page_number", F.col("page_number").cast(IntegerType()))
                display(parse_audio_response["dataframe"])
                all_dfs.append(parse_audio_response["dataframe"])
                
                # Add success log
                log_metadata.append({
                    "job_id": job_id,
                    "run_id": run_id,
                    "rag_app_name": rag_app_name_from_metadata_table,
                    "rag_app_source_id": rag_app_source_id_from_metadata_table,
                    "source_doc_path": os.path.join(adls_mnt_directory_path_from_metadata_table, filename),
                    "source_doc_name": filename,
                    "target_stage": "silver layer",
                    "target_path": rag_storage_acc_mount_point_from_metadata_table + silver_layer_target_path_from_metadata_table + "created_date=" + str(datetime.now().year) + "-" + str(datetime.now().month) + "-" + str(datetime.now().day),
                    "ingestion_status": "success",
                    "start_time": start_time,
                    "end_time": end_time
                })
            else:
                # Add failure log
                log_metadata.append({
                    "job_id": job_id,
                    "run_id": run_id,
                    "rag_app_name": rag_app_name_from_metadata_table,
                    "rag_app_source_id": rag_app_source_id_from_metadata_table,
                    "source_doc_path": os.path.join(adls_mnt_directory_path_from_metadata_table, filename),
                    "source_doc_name": filename,
                    "target_stage": "silver layer",
                    "target_path": rag_storage_acc_mount_point_from_metadata_table + silver_layer_target_path_from_metadata_table + "created_date=" + str(datetime.now().year) + "-" + str(datetime.now().month) + "-" + str(datetime.now().day),
                    "ingestion_status": "failure",
                    "error_details": parse_audio_response["status_details"],
                    "start_time": start_time,
                    "end_time": end_time
                })
    # ============================================================================
    # NEW CODE ENDS HERE
    # ============================================================================
        
    else:
        log_metadata.append({
                "job_id": job_id,
                "run_id": run_id,
                "rag_app_name" : rag_app_name_from_metadata_table,
                "rag_app_source_id" : rag_app_source_id_from_metadata_table,
                "source_doc_path" : os.path.join(adls_mnt_directory_path_from_metadata_table, filename),
                "source_doc_name" :filename,
                "target_stage" : "silver layer",
                "target_path": adls_mnt_directory_path_from_metadata_table,
                "ingestion_status": "failure",
                "error_details": "file not supported",
            })
